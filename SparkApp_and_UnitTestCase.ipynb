{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkApp:\n",
    "    \n",
    "    def load_data(self,customers,products,transactions):\n",
    "        '''\n",
    "            customers: file name in form of string and in csv format (e.g-'customers.csv')\n",
    "            products: file name in form of string and in csv format (e.g-'products.csv')\n",
    "            transactions: file name in form of string and in json format (e.g-'transactions.csv')\n",
    "            \n",
    "            The files should be in following schema:\n",
    "            customers:root\n",
    "                         |-- customer_id: string (nullable = true)\n",
    "                         |-- loyalty_score: integer (nullable = true)\n",
    "            products:root\n",
    "                         |-- product_id: string (nullable = true)\n",
    "                         |-- product_category: integer (nullable = true)\n",
    "            transactions:root\n",
    "                         |-- basket: array (nullable = true)\n",
    "                         |    |-- element: struct (containsNull = true)\n",
    "                         |    |    |-- price: long (nullable = true)\n",
    "                         |    |    |-- product_id: string (nullable = true)\n",
    "                         |-- customer_id: string (nullable = true)\n",
    "                         |-- date_of_purchase: string (nullable = true)\n",
    "\n",
    "            \n",
    "            creates a new spark session \n",
    "            returns the list of three dataframes [df_customers,df_products,df_transactions]\n",
    "        '''\n",
    "        \n",
    "        spark = self._create_a_SparkSession()\n",
    "        df_customers = spark.read.csv(customers,inferSchema = True , header = True)\n",
    "        df_products = spark.read.csv(products,inferSchema = True , header = True)\n",
    "        df_transactions = spark.read.json(transactions,multiLine=True)\n",
    "        \n",
    "        return [df_customers,df_products,df_transactions]\n",
    "    \n",
    "    def _create_a_SparkSession(self):\n",
    "        import pyspark\n",
    "        import findspark\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "\n",
    "        spark=SparkSession.builder.appName('Customer Details').getOrCreate()\n",
    "        return spark\n",
    "        \n",
    "    def flatten(self,df,column):\n",
    "        df = df.withColumn(column,explode(column))\n",
    "        return df\n",
    "    \n",
    "    def count_duplicate_rows(self,df):\n",
    "        '''\n",
    "            \n",
    "        '''\n",
    "        df = df.groupBy(df.columns)\\\n",
    "                .count()\\\n",
    "                .where(f.col('count') > 0)\n",
    "        return df\n",
    "        \n",
    "    def rename_column(self,df,original,new):\n",
    "        df = df.withColumnRenamed(original,new)\n",
    "        return df\n",
    "\n",
    "    def outer_join(self,df1,df2,col):\n",
    "        df_new =  df1.join(df2,on=col,how='outer')\n",
    "        return df_new\n",
    "    \n",
    "    def sort(self,df,col_1,col_2):\n",
    "        df = df.sort(f.col(col_1),f.col(col_2))\n",
    "        return df\n",
    "        \n",
    "    def load_output(self,customers,products,transactions):\n",
    "        '''\n",
    "            customers: file name in form of string and in csv format (e.g-'customers.csv')\n",
    "            products: file name in form of string and in csv format (e.g-'products.csv')\n",
    "            transactions: file name in form of string and in json format (e.g-'transactions.csv')\n",
    "            \n",
    "            The files should be in following schema:\n",
    "            customers:root\n",
    "                         |-- customer_id: string (nullable = true)\n",
    "                         |-- loyalty_score: integer (nullable = true)\n",
    "            products:root\n",
    "                         |-- product_id: string (nullable = true)\n",
    "                         |-- product_category: integer (nullable = true)\n",
    "            transactions:root\n",
    "                         |-- basket: array (nullable = true)\n",
    "                         |    |-- element: struct (containsNull = true)\n",
    "                         |    |    |-- price: long (nullable = true)\n",
    "                         |    |    |-- product_id: string (nullable = true)\n",
    "                         |-- customer_id: string (nullable = true)\n",
    "                         |-- date_of_purchase: string (nullable = true)\n",
    "                         \n",
    "            returns a new dataframe (df_output) with columns in order = ['customer_id','loyalty_score','product_id','product_category','purchase_count_per_product_id']\n",
    "\n",
    "        '''\n",
    "        \n",
    "        #loading the data first\n",
    "        df_customers,df_products,df_transactions = self.load_data(customers,products,transactions)\n",
    "        \n",
    "        #importing necessary functions\n",
    "        import pyspark.sql.functions as f\n",
    "        from pyspark.sql.functions import explode\n",
    "\n",
    "        #cleaning the df_products\n",
    "        df_products= df_products.drop('product_description')\n",
    "\n",
    "        # flattening the data from df_transaction \n",
    "        df_trans_flat = self.flatten(df_transactions,'basket').drop('date_of_purchase')\n",
    "\n",
    "        # rearranging counting the no of duplicate rows in df_trans_flat dataframe\n",
    "        df_trans_flat = df_trans_flat.select('customer_id','basket.*').drop('price')\n",
    "        \n",
    "        #counting the no of duplicate rows in df_trans_flat dataframe\n",
    "        df_trans_flat = self.count_duplicate_rows(df_trans_flat)\n",
    "\n",
    "        #renaming the column count to purchase_count_per_product_id\n",
    "        df_trans_flat = self.rename_column(df_trans_flat,'count','purchase_count_per_product_id')\n",
    "        \n",
    "        #creating an intermediate dataframe to outer merge the dataframes df_customers and df_trans_flat on customer_id\n",
    "        df_customer_trans = self.outer_join(df_customers,df_trans_flat,'customer_id')\n",
    "\n",
    "        #creating the final output dataframe by outer merging df_products and the intermediate dataframe on product_id\n",
    "        df_output = self.outer_join(df_customer_trans,df_products,'product_id')\n",
    "\n",
    "        #rearranging the output dataframe and sorting the data first on customer_id and then on product_id\n",
    "        df_output = df_output.select('customer_id','loyalty_score','product_id','product_category','purchase_count_per_product_id')\n",
    "        \n",
    "        df_output = self.sort(df_output,'customer_id','product_id')\n",
    "\n",
    "        #Printing the required data\n",
    "        return df_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_details = SparkApp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|           10|        P1|               1|                            2|\n",
      "|         C1|           10|        P3|               3|                            1|\n",
      "|         C2|          232|        P1|               1|                            1|\n",
      "|         C2|          232|        P2|               2|                            1|\n",
      "|         C3|           23|        P1|               1|                            1|\n",
      "|         C3|           23|        P2|               2|                            1|\n",
      "|         C4|           14|        P1|               1|                            1|\n",
      "|         C4|           14|        P2|               2|                            1|\n",
      "|         C5|           52|        P1|               1|                            1|\n",
      "|         C5|           52|        P2|               2|                            1|\n",
      "|         C6|           53|        P1|               1|                            1|\n",
      "|         C6|           53|        P2|               2|                            1|\n",
      "|         C7|          323|        P1|               1|                            1|\n",
      "|         C7|          323|        P2|               2|                            1|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_details.load_output('customers.csv','products.csv','transactions.json').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test cases with py.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PySparkUnitTestBase(unittest.TestCase):\n",
    "    def setUpClass(self):\n",
    "        spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Unit Testing in Pyspark Application')\\\n",
    "        .master('local[*]')\\\n",
    "        .getOrCreate()\n",
    "        self.spark = spark\n",
    "    \n",
    "    def tearDownClass(self):\n",
    "        self.spark.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PysparkUnitTest(PySparkUnitTestBase):\n",
    "    def _create_a_SparkSession(self):\n",
    "        import pyspark\n",
    "        import findspark\n",
    "        from pyspark.sql import SparkSession\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        spark=SparkSession.builder.appName('Customer Details').getOrCreate()\n",
    "        return spark\n",
    "    '''def test_data(self,df1, df2):\n",
    "        data1 = df1.collect()\n",
    "        data2 = df2.collect()\n",
    "        return set(data1) == set(data2)'''\n",
    "    def test_load_data_case(self,customer,product,transactions,output):\n",
    "        \n",
    "        spark = self._create_a_SparkSession()\n",
    "        expected_output_df = spark.read.csv(output,inferSchema=True,header=True)\n",
    "        output_df = SparkApp().load_output(customer,product,transactions)\n",
    "        output_df = output_df.withColumn('purchase_count_per_product_id',f.round(output_df[\"purchase_count_per_product_id\"]).cast('integer'))\n",
    "        #expected_output_df.show()\n",
    "        #output_df.show()\n",
    "        self.assertEqual( output_df.collect(),expected_output_df.collect())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|           12|        P1|               1|                            1|\n",
      "|         C1|           12|        P2|               2|                            1|\n",
      "|         C1|           12|        P3|               3|                            1|\n",
      "|         C2|           23|        P1|               1|                            1|\n",
      "|         C2|           23|        P2|               2|                            1|\n",
      "|         C3|          133|        P3|               3|                            1|\n",
      "|         C3|          133|        P4|               4|                            1|\n",
      "|         C4|           94|        P1|               1|                            1|\n",
      "|         C4|           94|        P4|               4|                            1|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|           12|        P1|               1|                            1|\n",
      "|         C1|           12|        P2|               2|                            1|\n",
      "|         C1|           12|        P3|               3|                            1|\n",
      "|         C2|           23|        P1|               1|                            1|\n",
      "|         C2|           23|        P2|               2|                            1|\n",
      "|         C3|          133|        P3|               3|                            1|\n",
      "|         C3|          133|        P4|               4|                            1|\n",
      "|         C4|           94|        P1|               1|                            1|\n",
      "|         C4|           94|        P4|               4|                            1|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PysparkUnitTest().test_load_data_case('customers_test-1.csv','products_test-1.csv','transactions_test-1.json','output_test-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|           12|        P1|               1|                            2|\n",
      "|         C1|           12|        P3|               3|                            1|\n",
      "|         C2|           23|        P2|               2|                            2|\n",
      "|         C3|          133|        P3|               3|                            2|\n",
      "|         C3|          133|        P4|               4|                            1|\n",
      "|         C4|           94|      null|            null|                         null|\n",
      "|         C5|          100|        P1|               1|                            2|\n",
      "|         C5|          100|        P4|               4|                            1|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|           12|        P1|               1|                            2|\n",
      "|         C1|           12|        P3|               3|                            1|\n",
      "|         C2|           23|        P2|               2|                            2|\n",
      "|         C3|          133|        P3|               3|                            2|\n",
      "|         C3|          133|        P4|               4|                            1|\n",
      "|         C4|           94|      null|            null|                         null|\n",
      "|         C5|          100|        P1|               1|                            2|\n",
      "|         C5|          100|        P4|               4|                            1|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PysparkUnitTest().test_load_data_case('customers_test-2.csv','products_test-2.csv','transactions_test-2.json','output_test-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
